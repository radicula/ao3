# Dead Dove: Do Not Eat (/jk)

I wanted to know about the history of the _Dead Dove: Do Not Eat_ tag on the
AO3 so spun up this little program for getting some stats on tag usage. It uses
[this unofficial AO3 API](https://ao3-api.readthedocs.io/). It's nothing too
fancy, just the result of an evening falling down a little rabbit hole.
Definitely leans Comp Sci 101 and could benefit from a makefile.

Tragically, it looks like the Unofficial API is very unofficial and not
actively maintained. One notable bug in the current version is that you can't
access search results for any search that returns >1,000 fics (which includes
Dead Dove). So, I've forked and am using my own version of it with this one
small bug fixed (you'll see that the import statement somewhat clumsily
reflects this as I didn't want to bother making a new venv). You'll be fine as
long as you're only looking at smaller tags, or else you can also use the
forked version.

## To run:

Start by installing `requirements.txt` using `pip install -r requirements.txt` or
  similar. This was written using `Python 3.8.10`. You'll probably want to use
a virtual environment of some kind.

The entire flow, to generate a CSV file, looks like this:
1. Run `python3 get-works.py`. You will be prompted from the command line to
   enter the tag you're interested in analyzing. Copy and paste it _exactly_
from AO3 and press Enter. Ignore the second optional prompt (just press Enter).
2. Run `python3 get-data.py <output filepath from step 1>`. Again, ignore the
   optional prompt.
3. Run `python3 json-to-csv.py <output filepath from step 2>`.

This will generate a number of files: if you find there are errors in your CSV
(perhaps due to funky formatting in a fic description that messes with the
comma delimitation), you can go back to the JSON generated by step 2.

This is designed to be able to restart midway upon failure. If step 1 fails
midway through, make note of what page number the failure occurred on and input
that page number on the second optional prompt when running `get-works.py` to
re-continue that data collection **from that page number** (so add 1 if that
page is already successfully collected). Be sure to check that you're not
adding duplicates; the `tqdm` bar isn't always accurate so you might want to
manually check the number of IDs that are already in the output file (there are
20 works on each page, so number of IDs currently saved / 20 + 1 = what you
should enter for the prompt).

Similarly, you can restart midway through after failure for step 2. You will enter
the work ID for the _last successfully processed work_ and collection will
restart on the next work after that.

**NOTE:** The rate limits on this unofficial API are *very low* so any sizeable
collection will take quite a while. I find I have to wait 5 minutes or so every
~100 requests.

